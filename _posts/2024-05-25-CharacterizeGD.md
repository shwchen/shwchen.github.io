---
layout: post
title: "Characterizing Graph Datasets for Node Classification: Homophily–Heterophily Dichotomy and Beyond"
date: "2024-05-05"
slug: "characterize-graph-datasets"
description: "Homophily describes the tendency for similar nodes to connect, while heterophily involves dissimilar nodes, challenging GNNs. Current homophily measures have drawbacks. This work introduces 'adjusted homophily', which meets more desirable properties, and proposes 'label informativeness' (LI) to better distinguish heterophily types. LI aligns well with GNN performance, proving useful for characterizing graph structure."
category: 
  - Social and Information Networks (cs.SI)
  - Discrete Mathematics (cs.DM)
  - Machine Learning (cs.LG)
  - Probability (math.PR)
tags:  # tags will also be used as html meta keywords.
  - Node Classification
  - Homophily
  - heterophily
  - NeurIPS'23
show_meta: true
comments: true
mathjax: true
gistembed: true
published: true
noindex: false
nofollow: false
hide_printmsg: false  # hide QR code, permalink block while printing.
summaryfeed: false  # show post summary or full post in RSS feed.
---

**Authors:** [Oleg Platonov](https://arxiv.org/search/cs?searchtype=author&query=Platonov,+O), [Denis Kuznedelev](https://arxiv.org/search/cs?searchtype=author&query=Kuznedelev,+D), [Artem Babenko](https://arxiv.org/search/cs?searchtype=author&query=Babenko,+A), [Liudmila Prokhorenkova](https://arxiv.org/search/cs?searchtype=author&query=Prokhorenkova,+L). \
**Venue:** 37th Conference on Neural Information Processing Systems (NeurIPS 2023). \
**Links:** [ArXiv](https://arxiv.org/abs/2209.06177), [OpenReview](https://openreview.net/forum?id=D4GLZkTphJ). \
**Code:** [Notebook](https://colab.research.google.com/drive/186KlV8PrWOq_woZVaRWGYC2B10vSm7S2?usp=sharing); [DGL implementation](https://docs.dgl.ai/en/2.0.x/generated/dgl.node_label_informativeness.html). \
**BibTex:** 
~~~r 
@inproceedings{Platonov2022CharacterizingGD,
  title={Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond},
  author={Oleg Platonov and Denis Kuznedelev and Artem Babenko and Liudmila Prokhorenkova},
  booktitle={Neural Information Processing Systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:256808725}
} 
~~~

# 1 Introduction

## 1.1 Preliminary

### 1.1.1 Homophily

Edges tend to connect *similar* nodes. For instance, users in social networks tend to connect to users with similar interests, and papers in citation networks mostly cite works from the same research area.

Some popular Homophily Measures: 

#### 1.1.1.1 Edge Homophily

Computes the fraction of edges that connect nodes of the same class. 

$$
  h_{edge} = \frac{|\{\{u, v\} \in E: y_u = y_v\}|}{|E|}
$$

#### 1.1.1.2 Node Homophily

Computes the fraction of neighbors of same class for all nodes.

$$
  h_{node} = \frac{1}{n}\sum_{v\in V}\frac{|\{\{u, v\} \in N(v): y_u = y_v\}|}{d(v)}
$$

#### 1.1.1.3 Class Homophily

Measures excess homophily compared to a null model where edges are independent of the labels.

$$
h_{class} = \frac{1}{C-1}\sum_{k=1}^{C}\left[ \frac{\sum_{v:y_v=k}|\{u\in N(v):y_u=y_v\}|}{\sum_{v:y_v=k}d(v)} - \frac{n_k}{n} \right]_{+}
$$

1 and 2 are sensitive to number of classes and their balance. 3 addresses the issue but only consider positive deviation from $$\frac{n_k}{n}$$ (neglecting heterophilous patterns) and does not consider variation of node degrees.

### 1.1.2 Heterophily

Edges tend to connect *dissimilar* nodes. For instance, in social networks, fraudsters rarely connect to other fraudsters, while in dating networks, edges often connect the opposite genders.

### 1.1.3 Desired Properties for Homophily Measures

>Definition: A Configuration Model is constructed by taking n nodes, assigning each node a degree v, and then ramdomlly connecting them to form a graph.

Given a homophily measure $$h$$:

#### 1.1.3.1 Maximal Agreement (Max)

It satisfies *maximal agreement* if for any graph $$G$$ where all edges connect nodes of the same class, then $$h(G)$$ should return the max score, and a score less than max score otherwise.

#### 1.1.3.2 Minimal Agreement (Min)

The opposite of Maximal Agreement.

#### 1.1.3.3 Asymptotic Constant Baseline (ACB)

For large graphs generated by the configuration model, the measure consistently approximates a fixed value with high probability, ensuring reliable comparisons across datasets.

#### 1.1.3.4 Empty Class Tolerance (ECT)

It is *Empty Class Tolerant* if it is defined and it does not change when we introduce an additional dummy label that is not present in the data.

#### 1.1.3.5 Monotonicity (Mono)

It is *monotone* if it is empty class tolerant, increases when we add an edge between two nodes of the same class (except for perfectly homophilous graphs) and decreases when we add an edge between two nodes of different classes (except for perfectly heterophilous graphs).

## 1.2 Motivation of the Paper

| Measures | Max | Min | ACB | ECT | Mono |
| :--- | :---: | :---: | :---: | :---: | :---: |
| Edge | yes | yes | no | yes | yes|
| Node | yes | yes | no | yes | no |
| Class | yes | no | no | no | no |
| Adjusted | yes | no | yes | yes | no |

Several Homophily Measures used prevalently do not satisfy the desired properties we just discussed. The authors claim that `Adjusted Homophily is a better choice as it meets most of the requirements.`{:.yelhglt}

Heterophilous graph datasets can have various connectivity patterns and some of them are easier for GNNs than others. `The authors proposed a new graph property, named Label Informativeness (LI), which characterizes how much information the neighbor's label provides about the node's label, to complement Adjusted Homophily by distinguishing different homophilous patterns.`{:.yelhglt} Empirical evidence testifies that LI better explains GNNs' performance than Homophily Measures.

# 2 Method and Theory

## 2.1 How to Adjust Homophily

Steps to make adjusted homophily:

**Step 1.** Start with $$h_{edge}$$.

$$
  h_{edge} = \frac{|\{\{u, v\} \in E: y_u = y_v\}|}{|E|}
$$

**Step 2.** Subtract the the Expected Value, that is the expected homophily if edges were ramdom assigned. The probability that a given edge will be connected to a class k node is $$\frac{\sum_{v:y_v=k}d(v)}{2\lvert E\rvert}$$

$$
  h_{adj} = h_{edge} - \sum_{k=1}^C \frac{D_k^2}{4|E|^2}
$$

**Step 3.** Dividing by a a term that accounts for the distribution of class labels, a way of normalization to ensure compatibility.

$$
  h_{adj} = \frac{h_{edge} - \sum_{k=1}^C \bar{p}(k)^2}{1 - \sum_{k=1}^C \bar{p}(k)^2}
$$

The term $$\bar{p}(k) = \frac{D_k}{2\lvert E\rvert}$$ is known as the assortivity coefficient. 

Note:

Squaring emphasize the the variance and distribution of edges in graph, ensuring the expected homophily measure is sensitive to the actual structure of the graph.

## 2.2 How Edge-wise Homophily relates to Classification Evaluation Metrics

>Definition: Each elements of a Class Adjacency Matrix indicates the number of edges connecting nodes of class i and nodes of class j.

For each edge $$(u, v)$$, if $$y_u$$ is a true label while $$y_v$$ is a predicted label, then any classification evaluation measure applied to this dataset is a measure of Homopholy.

Clearly, the Edge Homophily relates to accuracy, whereas the Adjusted homophily corresponds to both Cohen’s Kappa and Matthews coefficient, on such a dataset.

## 2.3 Characterize Heterophilous Patterns by Label Informativeness (LI)

Adjusted Homophily captures the absense of homophily in heterophilous graphs, it cannot identify which type these graphs belong to. 

The intuition behind LI is simple, the easier it becomes to predicting a node's label by knowing its neighbors', the more informative its neighbors are. Formally, if we sample a edge $$(a, b) \in E$$, the LI is the normalized mutual information of the two random variables $$y_a$$ and $$y_b$$:

$$
  LI \in [0, 1] = I(y_a, y_b) / H(y_a)
$$

If knowing $$y_b$$ completely removes uncertainty about $$y_a$$, then LI is 1. If $$y_a \perp \!\!\! \perp y_b$$, Li is 0.

# 3 Experiment and Analysis

# 4 Thought and Discussion

